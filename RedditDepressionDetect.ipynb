{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Capstone Project\n",
    "Elizabeth Cho, David Gonzalez-Maldonaldo, Ruge Ding, Kai Pang , Vy Huynh \n",
    "# Introduction and Background\n",
    "## Overview\n",
    "Depression is not to be taken lightly. According to data published by the National Institute of Mental Health, 17.3 million adults, (equivalent to 7.1% of all adults) had at least one major depressive episode in 2017. Furthermore, major depression is most prevalent amongst young adults as it affects 13.1% of age 18-25 population, and is the second leading cause of death for people aged 15-24. Early detection of depressive tendencies may greatly improve many peoples’ quality of life and lead to timely treatment.\n",
    "\n",
    "As social media grows in popularity, more and more people invest much of their time and energy into expressing themselves online, especially in ways that they may not express themselves to people face to face. Therefore, there is much information online regarding a person’s habits and interests that may help people be aware of, or seek treatment for, depressive traits. \n",
    "\n",
    "Formal definition of Major Depression (source from National Institute of Mental Health)\n",
    "\n",
    "‘Major depression is characterized by...an overwhelming feeling of sadness or a loss of interest and pleasure in most usual activities. The other symptoms that are associated with major depression include...feelings of worthlessness or excessive and inappropriate guilt, recurrent thoughts of death and suicidal ideation with or without specific plans for committing suicide, and cognitive difficulties, such as, diminished ability to think, concentrate and make decisions. Social, occupational, educational, or other important functioning is also impacted.’\n",
    "\n",
    "Reddit is an anonymous forum with more than 200million users. Each user can post or comment in “subreddits”, certain sections of the forum that are dedicated to different interests. Because it is anonymous and allows people to pursue specific topics, many people express themselves freely, which makes it a useful platform to analyze honest internet habits. However, when using social media as a platform, one must consider the limitations of the data.\n",
    "\n",
    "Although depression can affect any person regardless of age, race, or location, Reddit users in no way accurately represent the global population. Studies have found that nearly 70% of Reddit users are male, and 93% of users are under 50 years old. In fact, 64% of users are between 18-26. It, therefore, comes as little surprise that more than 50% of users are currently students. On top of this, nearly 60% of users are from the United States, and 70% of those users are non-Hispanic white adults.\n",
    "\n",
    "In short, the average Reddit user is a white male college student from the United States. From this conclusion, it is necessary to consider how this may affect the data, especially as it relates to depression. As mentioned above, depression is most prevalent in those aged 18-25 and is also common among those who identify as white. Therefore we can deduce that depressive traits may be much more common on Reddit than in the general population. \n",
    "We may also have to consider other possible details about the average Reddit user based on these traits. For example, given that the majority of Reddit users have received higher education, users may be more affluent than the average citizen. All of these specifics indicate a very skewed demographic, which may affect the accuracy or application of our dataset. To counteract this, we will have to be careful to address the limits of our demographic when analyzing and utilizing our data.\n",
    "\n",
    "In order to achieve our goal, we need to train a Machine Learning model and use the model to predict if an individual user is depressed. ML is the scientific study of algorithms and statistical models that computer systems use to effectively perform a specific task without using explicit instructions, relying on patterns and inference instead. The model trained by machine learning algorithms, therefore, allows us to discover the patterns and results that we cannot obtain otherwise. In our project, one supervised machine learning algorithm, SVM, will be used to build a model for depression tendencies for Reddit users. The data set we are going to train our model with have dozens of karma scores under different subreddits and depression indicator of individual users. We will separate the data into training data and test data to find the best parameters for our SVM model by continuously verifying our model with test data. When our model achieves a reasonable accuracy, we will use the whole data set to train it again with the best parameters and use the model to predict the depression indicators of users from two different subreddit. If there is a systematic difference (the total depression rates) between two groups of users and it agrees with our expectation, we consider our project a success.\n",
    "\n",
    "An SVM model is, simply put, one that linearly divides the data into two sections. As shown in the images above, the idea is that the data contains two different subsections, between which the model can hopefully “draw a line” that is then used to predict whether future datapoints are on one side or the other. The first diagram shows how the datasets can intersect, therefore lowering the accuracy of the model, but it is still a strong predictor of future points.\n",
    "\n",
    "<img src='svm1.png'/>\n",
    "<img src='svm2.png'/>\n",
    "\n",
    "Within reddit there are several depression support communities, such as ‘r/depression’ and ‘r/SuicideWatch’, where primarily people with depression post and comment. Our machine learning model is based on analyzing the post and comment activity of people active on those subreddits and comparing this activity to that of others. We will then hopefully be able to draw conclusions about which other subreddits tend to be populated by people with depressive tendencies, and whether a user with similar activity history may want to seek support.\n",
    "\n",
    "\n",
    "Sources: \n",
    "https://www.nimh.nih.gov/index.shtml\n",
    "https://www.techjunkie.com/demographics-reddit/\n",
    "https://appliedmachinelearning.blog/2017/03/09/understanding-support-vector-machines-a-primer/\n",
    "https://towardsdatascience.com/demystifying-support-vector-machines-8453b39f7368\n",
    "\n",
    "## Research Question:\n",
    "Given a summary of thousands of reddit.com user histories, is it possible to train machine learning models that can classify members of certain communities based solely on their activity on the social networking site with acceptable levels of accuracy?\n",
    "\n",
    "## Background and references to other similar projects.\n",
    "\n",
    "### Detecting depression stigma on social media: A linguistic analysis\n",
    "\n",
    "Similar to our research question to come up with more efficient solutions to reduce depressive stigma in social media, this study aimed to use linguistic analysis to see if it is relevant and efficient in detecting depressive traits. Namely, by collecting and analysing keywords from more than 15000+ Weibo posts, this study uses content analysis as well as various algorithms such as Support Vector Machine, Multilayer Perceptron Neural Networks, Random Forest to essentially categorize each individual posts into 1) contains depressive traits 2) neutral. Quoted from science.direct.com, below is an excerpt that discusses the numerical results of the study:\n",
    " \n",
    "967 of 15,879 posts (6.09%) indicated depression stigma. 39.30%, 15.82%, and 14.99% of them endorsed the stigmatizing view that “People with depression are unpredictable”, “Depression is a sign of personal weakness”, and “Depression is not a real medical illness”, respectively. Second, the highest F-Measure value for differentiating between stigma and non-stigma reached 75.2%. The highest F-Measure value for differentiating among three specific types of stigma reached 86.2%.\n",
    "\n",
    "In short, what academics learnt from this study is that linguistic analysis to detect depressive traits is plausible and beneficial to improve the performance of depressive stigma reduction programs.\n",
    "\n",
    "\n",
    "Source: https://www.ncbi.nlm.nih.gov/pubmed/29510353\n",
    "### Facebook Use Links to Depressive Symptoms\n",
    "A study completed by researchers at the University of Houston has linked frequent Facebook use to symptoms such as depressed feelings and excessive self-comparisons. Researcher Mai-Ly Steers explains that social media may not cause depression, but “lots of time on Facebook” correlates to these symptoms, and may exacerbate emotional difficulties already present in users. This correlation suggests that frequent Facebook use can indicate depressive tendencies, and so can be analyzed in order to identify these traits. Although Facebook and Reddit are very different social media platforms, Steer’s research shows that how people use social media can reflect the state of their mental health, something that we are pursuing more specifically and with the direct purpose of identifying people who may need help.\n",
    "\n",
    "Source: https://www.houstonpublicmedia.org/articles/news/2015/04/20/59497/uh-study-spending-time-on-facebook-can-trigger-depressive-symptoms-2/\n",
    "\n",
    "## Hypothesis\n",
    "We hypothesize that users with above average participation in `r/depression` will exhibit depressive tendencies such that a model trained on their behavior will be able to recognize similar 'depressive' tendencies in the users of other similar communities such as `r/SuicideWatch`\n",
    "\n",
    "# Datasets:\n",
    "The processed datasets can be viewed here:\n",
    "https://drive.google.com/open?id=1_QBuPkpiHW_rCfl5xrFeR9leOtPn94SS\n",
    "\n",
    "Our raw dataset is too large to be uploaded, for access please contact David Gonzalez-Maldonado (dag057@ucsd.edu)\n",
    "\n",
    "# Data Description\n",
    "We uesd reddit.com's 'PRAW' API to collect publicly available data based on the following methodology:\n",
    "- First, scrape the most popular posts on reddit (as defined by having the highest karma --similar to likes on Facebook-- score in a given timeframe) and gather a list of all users that commented on those posts.\n",
    "- Once a sufficiently large list of distinct users has been gathered (~6,000 per batch) they will be compared to our existing database to make sure we do not have any duplicates.\n",
    "- The batch of users will then be distributed amongst 30 processes that will run in parallel to each other where each process will scrape the entire post history of a given user and store it as a .json file. We can be sure that our parallel programs will never interfere with each other due to the fact that we guaranteed that all users in our batch were distinct and not already in our dataset; thus there will never be two processes that attempt to modify the same file at once.\n",
    "- After all processes finish, we will repeat using the latest set of most popular posts.\n",
    "\n",
    "Below is a sample of a single entry in our raw dataset:\n",
    "\n",
    "```\n",
    "{\"body\": \"I don't know if it's available in blender but a histogram will allow you to see if an image is properly exposed or not.\\n\\n[This article seems to explain it pretty well.](https://digital-photography-school.com/how-to-read-and-use-histograms/)\", \"created_utc\": 1538675047.0, \"distinguished\": null, \"edited\": false, \"id\": \"e75xwxs\", \"is_submitter\": false, \"link_id\": \"t3_9l9n3c\", \"parent_id\": \"t1_e75ob1t\", \"permalink\": \"/r/blender/comments/9l9n3c/chicken_1_inktober_day_3_roasted_timelapse/e75xwxs/\", \"score\": 2, \"stickied\": false, \"subreddit_id\": \"t5_2qim4\", \"subreddit\": \"r/blender\", \"ups\": 2, \"downs\": 0, \"controversiality\": 0, \"gilded\": 0}\n",
    "```\n",
    "\n",
    "Our final dataset contains 5,506 rows (users) and 36 columns. Columns 1 to 35 correspond to a user's karma score in the 35 subreddits that link most significantly to either r/happy or r/depression (ordered alphabetically). The final column is our target, it is called 'Depressed' and is a binary flag symbolizing whether a given user is a member of our 'Depressed' (1) group or our 'Not Depressed' (0) group (this is what our SVM model will be predicting).\n",
    "\n",
    "At this point we find it incumbent to note that reddit.com prides itself on a Laissez-faire approach to content moderation, anything that is not explicitly illegal in the United States is therefore very likely to be allowed. An unfortunate side-effect of such a generous adherence to freedom of speech is that a not-insubstantial portion of their content and (by extension) the data we will be presenting deals with topics that are crude, offensive or otherwise inappropriate in most contexts. We made the decision not to censor such content in the interest of academic honesty, however, we feel a responsibility to explicitly condemn the small subset of communities that spew racist, misogynistic, homophobic/transphobic, or otherwise hatefull content.\n",
    "\n",
    "# Dimensionality Reduction\n",
    "A greater number of features means that on average, one users have less than one feature. The actual situation is even worse. Even though most humans can only imagine 1D, 2D and 3D, it’s not hard to tell a cube has much more possible placement than a 2D square. As the number of dimensions grow to 6097, over 99.9% of the vector space is essentially empty. The SVM model training results based on original data of 6097 dimensions are bad without any doubt: the model just randomly guessed the depression indicator.\n",
    "\n",
    "Therefore, dimension reduction is a must if we want to let our model do any actual work.\n",
    "\n",
    "<img src='DR1.png'/>\n",
    "<img src='DR2.png'/>\n",
    "These graphs illustrate the “curse of dimensionality”. Our dataset had many dimensions, but the “curse” states that as the number of dimensions increases, the number of datapoints required increases exponentially. Simply put, we did not have the extreme amount of data needed to represent every dimension, so an important part of our data processing was to cut down on the data we used for the sake of our model’s accuracy.\n",
    "\n",
    "Sources: \n",
    "https://towardsdatascience.com/curse-of-dimensionality-2092410f3d27\n",
    "https://eranraviv.com/curse-of-dimensionality/\n",
    "\n",
    "\n",
    "# Data Cleaning\n",
    "One of the main upsides of using an officially supported API to perform our data gathering as opposed to scraping raw html is that the data the API provided us was already in a reasonably clean format: the API would provide a 'Comment' object (analogous to a C struct) that had a field for every bit of data associated with that comment (body, several timestamps, karma score, subreddit, etc). The only 'cleaning' required was to create a python dictionary out of the fields and then save that as a json file as shown above. Below is the code we used to gather the entire user post history for a given user. Showing how simple it was to get clean data thanks to the praw reddit API.\n",
    "``` python\n",
    "from dagm.utils.driveDrop import DriveDrop\n",
    "from dagm.utils import folderManager\n",
    "import argparse as ap\n",
    "import time\n",
    "import praw\n",
    "import json\n",
    "\n",
    "def gatherUserData(userName, dd, reddit, ts, tempFile):\n",
    "\n",
    "\tcurrentUser = reddit.redditor(userName)\n",
    "\tresults = dd.findFile(f'{userName}.json')\n",
    "\tif len(results) != 1:\n",
    "\t\tts = 0\n",
    "\t\tcomments = []\n",
    "\telse:\n",
    "\t\tfid, mtype = results[0]\n",
    "\t\tdd.getFile(fid, tempFile)\n",
    "\t\toutFile = open(tempFile)\n",
    "\t\tcomments = json.loads(outFile.read())\n",
    "\t\toutFile.close()\n",
    "\tfor uComment in currentUser.comments.new(limit=None):\n",
    "\t\tif float(uComment.created_utc) < ts:\n",
    "\t\t\tbreak\n",
    "\t\tcurrentComment = {\n",
    "\t\t\t'body': uComment.body,\n",
    "\t\t\t'created_utc': uComment.created_utc,\n",
    "\t\t\t'distinguished':uComment.distinguished,\n",
    "\t\t\t'edited': uComment.edited,\n",
    "\t\t\t'id': uComment.id,\n",
    "\t\t\t'is_submitter': uComment.is_submitter,\n",
    "\t\t\t'link_id': uComment.link_id,\n",
    "\t\t\t'parent_id':uComment.parent_id,\n",
    "\t\t\t'permalink': uComment.permalink,\n",
    "\t\t\t'score': uComment.score,\n",
    "\t\t\t'stickied': uComment.stickied,\n",
    "\t\t\t'subreddit_id': uComment.subreddit_id,\n",
    "\t\t\t'subreddit': uComment.subreddit_name_prefixed,\n",
    "\t\t\t'ups': uComment.ups,\n",
    "\t\t\t'downs': uComment.downs,\n",
    "\t\t\t'controversiality': uComment.controversiality,\n",
    "\t\t\t'gilded': uComment.gilded}\n",
    "\t\tcomments.append(currentComment)\n",
    "\toutFile = open(tempFile,'w', encoding='ASCII')\n",
    "\toutFile.write(json.dumps(comments))\n",
    "\tdd.writeFile(tempFile,mimeType='application/json', name=f'{userName}.json', overwrite=True)\n",
    "\toutFile.close()\n",
    "```\n",
    "\n",
    " # Data Pre-Processing / Visualization\n",
    " \n",
    "Given the relatively huge size of our dataset (100+Gbs), the bulk of our preprocessing involved reducing the size of our dataset to something that can be used to train conventional machine learning models running on personal computers. To this end our first preprocessing step was to reduce the granularity of our dataset from the level where each data point is an individual user comment to having each data point be a summary of the user's overall karma in each subreddit. \n",
    "While the previous step reduced our dataset from several million entries to about half a million entries, we still had to deal with the fact that our dataset had 132,888 individual features (subreddits). We initially tried to let Principal Component Analysis do the job for us, however, we fouind that the results were not imporving significantly so we had to take a more involved approach: To do this we decided to discard to discard all subreddits (features) whose sum of karma equaled less than 6,238.* This reduced the dimensionality of our dataset to 6,170 which was still far larger than what would be acceptable for machine learning models, however, at this point the dataset was manageable enough for more sophisticated analysis. \n",
    "\n",
    "An unfortunate side effect of using such a gigantic dataset is that many conventional visualization techniques are not nearly as evocative. For instance, blindly plotting a histogram of our entire dataset in order to attempt to visualize patterns is substantially less effective when there are six-thousand categories. For this reason we were required to use more sophisticated (and unfortunately less visually appealing) methods, mainly graph theory, to visualize and engage with our dataset. Neverthelless here are some conventional visualizations we found usefull:\n",
    "\n",
    "<img src='Total_Karma_by_Subbreddit.png', width=600px/>\n",
    "<img src='KarmaPlotAll.png', width=600px/>\n",
    "_Clearly the karma distribution of reddit is focused primarily in a few very popular communities_\n",
    "<img src='KarmaPlotPruned.png', width=600px/>\n",
    "_Even when discarding about 95% of all communities, we were able to capture the vast majority of the content_\n",
    "\n",
    "# Data Analysis / (Text-based) Graph 'Visualizations'\n",
    "\n",
    "We used this reduced dataset to analyze co-activity patterns amongst subreddits in order to build a graph of the social network where each vertex is a subreddit. For any two vertices (vertex A, and vertex B) the edge connecting them (AB) is computed as follows: for every user in our database, we compute how strong a presence they have in subreddit A and B (as measured by how many standard deviations away from the average their karma is in a given subreddit), we then multiply these two values together to calculate the users 'co-activity' in both communities, we finally sum all of the co-activities for all users and then use that value as the weight between the subreddit A and subreddit B. We then computed what the distribution of all links in our graph was and used these statistics to cut our graph in various ways (for instance, keeping only edges whose weights are at least two standard deviations above the average weight value). While this method is admittedly crude, it proved to be surprisingly effective at finding clusters of similar communities for instance:\n",
    "\n",
    "`\n",
    "r/sandiego` links significantly with: `r/surfing` `r/orangecounty` `r/UCSD` `r/GalaxyNote8` `r/Bass` `r/linuxadmin` `r/baseballcirclejerk` `r/SCREENPRINTING` `r/comiccon` `r/cripplingalcoholism` `r/rollerblading` `r/Hawaii` `r/drums` `r/kingdomcome` `r/BostonBruins` `r/ElectricSkateboarding` `r/massage` `r/BigIsland` `r/LeagueofFailures` `r/metroidvania`\n",
    "\n",
    "\n",
    "Similarly:\n",
    "\n",
    "`\n",
    "r/LosAngeles` links significantly with: `r/California` `r/LAFC` `r/starbucks` `r/Maher` `r/longbeach` `r/LAlist` `r/trailers` `r/orangecounty` `r/AskLosAngeles` `r/Dodgers` `r/Disneyland` `r/ucla` `r/germanshepherds` `r/Coachella` `r/fyrefestival` `r/snowboarding` `r/LosAngelesRams` `r/losangeleskings` `r/WeAreTheMusicMakers` `r/Economics` `r/surfing` `r/postmates` `r/vfx` `r/comedy` `r/audioengineering` `r/civilengineering` `r/Hawaii` `r/nin` `r/criterion` `r/flying` `r/GenderCritical` `r/InlandEmpire` `r/LasVegas` `r/parrots` `r/LatinoPeopleTwitter` `r/infj` `r/asianamerican` `r/traderjoes` `r/MusicBattlestations` `r/justicedemocrats` `r/PointlessStories` `r/the_darnold` `r/psychotherapy` `r/eos` `r/SheLikesItRough`\n",
    "\n",
    "\n",
    "San Diego and Los Angeles are both coastal cities in Southern California so the fact that we were able to find that they both had strong links to the surfing and to Orange County communities as well as their respective UC campuses is fairly strong evidence that our graph is in fact capturing valid co-activity patterns. Once we were confident in the reliability of the graph we used it to see whether the users that participated in our hypothesized 'Depression' communities where indeed displaying depressive tendencies in significant numbers. It is saddening to note that we found very little evidence to reject our hypothesis as members of r/depression had statistically significant coactivity with communities oriented around mental health issues, self-harm/suicide focused communities, racist and political extremism communities, amongst others :\n",
    "\n",
    "`\n",
    "r/depression` links significantly with: `r/socialanxiety` `r/bipolar` `r/Anxiety` `r/selfharm` `r/wowthanksimcured` `r/BDSMAdvice` `r/SuicideWatch` `r/2meirl4meirl` `r/TheDepthsBelow` `r/ForeverAlone` `r/socialskills` `r/EngineeringStudents` `r/r4r` `r/mentalhealth` `r/ADHD` `r/ShouldIbuythisgame` `r/OCPoetry` `r/TrueOffMyChest` `r/sad` `r/needadvice` `r/depression_memes` `r/toastme` `r/introvert` `r/amiugly` `r/lonely` `r/Needafriend` `r/weed` `r/antinatalism` `r/BreakUps` `r/AustralianCattleDog` `r/depression_help` `r/misanthropy` `r/intj` `r/LilPeep` `r/lostgeneration` `r/nihilism` `r/SourceFed` `r/frenworld` `r/altgonewild` `r/CPTSD` `r/horizon` `r/knives` `r/BPD` `r/Target` `r/Makeup` `r/thanksimcured` `r/HealthAnxiety` `r/zen` `r/biglittlelies` `r/DevilMayCry` `r/AsianLadyboners` `r/realasians` `r/McDonalds` `r/gaybears` `r/palegirls` `r/shield` `r/BelleDelphine` `r/zombies` `r/BillAndPhil` `r/hapas` `r/Overwatch_Porn` `r/Logic_301` `r/vfx` `r/Warhammer30k`\n",
    "\n",
    "\n",
    "And on a lighter note:\n",
    "\n",
    "`\n",
    "r/happy` links significantly to:`r/MakeNewFriendsHere` `r/crafts` `r/goldenretrievers` `r/FreeCompliments` `r/happycryingdads` `r/knitting` `r/Eminem` `r/MensRights` `r/nostalgia` `r/TheBluePill` `r/TrueOffMyChest` `r/Connecticut` `r/navyseals` `r/slowcooking` `r/pitbulls` `r/HealthyFood` `r/offmychest` `r/showerbeer` `r/Superbowl` `r/lastimages` `r/AbandonedPorn` `r/bisexual` `r/exmormon` `r/rant` `r/uglyduckling` `r/drawing` `r/thebachelor` `r/HVAC` `r/thefighterandthekid` `r/ConfusedBoners` `r/somethingimade` `r/spotify` `r/FoundPaper` `r/AskMenOver30` `r/stopdrinking` `r/CorporateFacepalm` `r/distension` `r/altgonewild` `r/pornfree` `r/cumsluts` `r/intrusivethoughts` `r/classiccars` `r/trashpandas` `r/BossFights` `r/Bioshock` `r/fightsub` `r/trance` `r/Assistance` `r/chelseafc` `r/snackexchange` `r/poppunkers` `r/onetruegod` `r/opiates` `r/RealAhegao` `r/AlisonBrie` `r/Dogberg` `r/workgonewild` `r/YouSeeComrade` `r/exchristian` `r/Texans` `r/ConvenientCop` `r/shittylimos` `r/everymanshouldknow` `r/fifthworldproblems` `r/misc` `r/facebookwins` `r/sugarfreemua` `r/HaveWeMet` `r/castles` `r/lookatmydog` `r/MarkMyWords` `r/BSA` `r/DrunkOrAKid` `r/panelshow` `r/IndoorGarden` `r/drones` `r/Idaho` `r/FrugalFemaleFashion` `r/VRchat` `r/AusLegal` `r/TampaBayLightning` `r/TheOA` `r/NewProductPorn` `r/legodeal` `r/projecteternity` `r/NewOrleans` `r/RepLadies` `r/RealmRoyale` `r/OldSchoolCelebs` `r/FallOutBoy` `r/asianamerican` `r/Sumo` `r/saab` `r/Kings_Raid` `r/UCONN` `r/RedPillWomen`\n",
    "\n",
    "<img src='RedditGraph.png', width = 600px/>\n",
    "_A visualization of our Reddit graph. Once again, note how_\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "At this point we felt comfortable enough to begin building the dataset we eventually actually feed our classifiers, we defined membership into the 'Depressed' group as having an above average karma score in the 'r/depression' subreddit; similarly, we defined membership in the 'Not Depressed' group as having an above average karma score in 'r/happy.' We originally had 2825 members in our 'Depressed' group and 2785 members in our 'Not Depressed' group. The intersection between the two groups was 52; since this was such a small subset (~0.1% of our dataset) of our dataset we felt comfortable discarding these users in order to have two purely disjoint sets as required by binary classification. It is worth noting that, due to our data collection methodology (see above), the fact that we ended up with an almost 50/50 split of 'Depressed' vs 'Not Depressed' users in our dataset is not an artificial characteristic that we manufactured in our dataset (for example by directing our to data scrapper explicitly mine equal number of users from each community) but rather a fairly representative sample of the reddit community as a whole (assuming users from each community are equally likely to post on the most popular subreddits). We were therefore able to reduce the number of entries in our dataset from over half a million users to a little more than five thousand. Similarly, we used our graph to reduce the number of features by including only substitute that were N distance from either r/happy and r/depressed. We note that using this method of dimensionality reduction we were able to achieve higher accuracy than with PCA.\n",
    "\n",
    "*Note: 6,238 corresponds to subs with karma greater than: averageKarma - 0.01 x standardDeviation. While our distribution is clearly not normal, making our cutoff point a function of the standard deviation seemed like a marginally more principled approach than manually choosing a cutoff,  nevertheless, it should still be considered a fairly arbitrary cutoff. We felt comfortable with this as we were only interested in removing trivial/inactive subreddits.\n",
    "\n",
    "\n",
    "*Our entire processing stream:*\n",
    "\n",
    "``` python\n",
    "from dagm.utils.progressBar import printProgressBar\n",
    "\n",
    "\n",
    "def countKarmaAndParticipants(subredditList:str, userSummaries:str, outFile:str):\n",
    "\t\"\"\"Parses the userSummaries file to count how much total karma a subreddit \n",
    "\thas and how many participants\n",
    "\t\n",
    "\tArgs:\n",
    "\t    subredditList (str): file path to the file containing a list of all subreddits\n",
    "\t    userSummaries (str): filepath to user summaries\n",
    "\t    outFile (str): file path for output file\n",
    "\t\"\"\"\n",
    "\tsubreddits = sorted(open(subredditList).read().splitlines())\n",
    "\tsubCounters = {sub:[0,0] for sub in subreddits}\n",
    "\tusers = [u.split('\\t') for u in open(userSummaries).read().splitlines()]\n",
    "\tcounter = 0\n",
    "\toutput = open(outFile,'w+')\n",
    "\tfor user in users:\n",
    "\t\tfor sub in user[1:]:\n",
    "\t\t\tsubScore = sub.split(' ')\n",
    "\t\t\tsubreddit = subScore[0]\n",
    "\t\t\tscore = int(subScore[1])\n",
    "\t\t\tsubCounters[subreddit][0] += score \t# cumulative karma count\n",
    "\t\t\tsubCounters[subreddit][1] += 1 \t\t# users active in subredit\n",
    "\t\tif counter % 500 == 0:\n",
    "\t\t\tprintProgressBar(counter, len(users))\n",
    "\t\tcounter += 1\n",
    "\n",
    "\toutput.write('\\n'.join([' '.join([sub, str(subCounters[sub][0]),str(subCounters[sub][1])]) for sub in subCounters]))\n",
    "\n",
    "\n",
    "def calculateStats(intermediateSubSummaries:str, userSummaries:str, outfile:str):\n",
    "\t\"\"\"Calculates the mean and standard deviation for each subreddit\n",
    "\t\n",
    "\tArgs:\n",
    "\t    intermediateSubSummaries (str): Path to file containing the total karma\n",
    "\t    and participants count for each subreddit\n",
    "\t    userSummaries (str): path to file containing the user summaries (output \n",
    "\t    from countKarmaAndParticipants)\n",
    "\t    outfile (str): where to save the resulting csv\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport numpy as np\n",
    "\tfrom multiprocessing.dummy import Pool as ThreadPool \n",
    "\tfrom functools import reduce\n",
    "\timport time\n",
    "\tintermediate = pd.read_csv(intermediateSubSummaries, index_col=0)\n",
    "\tintermediate['Mean'] = intermediate['Total Karma'] / intermediate['Total Participants']\n",
    "\tintermediate['SD'] = np.zeros(len(intermediate))\n",
    "\n",
    "\tprint('Loading User Summaries')\n",
    "\tuserSummaries = [list(map(lambda x: (lambda y: (y[0], int(y[1])))(x.split(' ')),[subScore for subScore in l.split('\\t')[1:] if subScore.startswith('r')])) for l in open(userSummaries).read().splitlines()]\n",
    "\tprint('Loaded')\n",
    "\tprint(len(userSummaries))\n",
    "\t\n",
    "\tdef threadFun(userSummary:str):\n",
    "\t\toutput = {}\n",
    "\t\tfor sub, score in userSummary:\n",
    "\t\t\tif sub not in output:\n",
    "\t\t\t\toutput[sub] = 0\n",
    "\t\t\toutput[sub] += (score - intermediate.loc[sub]['Mean'])**2\n",
    "\t\treturn output\n",
    "\n",
    "\tpool = ThreadPool(2)\n",
    "\n",
    "\tdef mergeResults(x,y):\n",
    "\t\tfor sub in y:\n",
    "\t\t\tif sub in x:\n",
    "\t\t\t\tx[sub] += y[sub]\n",
    "\t\t\telse:\n",
    "\t\t\t\tx[sub] = y[sub]\n",
    "\t\treturn x\n",
    "\tstart_time = time.time()\n",
    "\tresults = reduce(mergeResults, pool.map(threadFun, userSummaries))\n",
    "\t# close the pool and wait for the work to finish\n",
    "\tpool.close()\n",
    "\tpool.join()\n",
    "\tfor sub in results:\n",
    "\t\tintermediate.at[sub, 'SD'] = np.sqrt(results[sub]/intermediate.loc[sub]['Total Participants'])\n",
    "\tintermediate.to_csv(outfile)\n",
    "\tprint(time.time() - start_time)\n",
    "\n",
    "\n",
    "def pruneSubs(subSummaries:str, nSD:int, outfile:str):\n",
    "\t\"\"\"Removes all subreddits that are not at least nSD from the mean\n",
    "\t\n",
    "\tArgs:\n",
    "\t    subSummaries (str): path to the subSummaries containing mean and sd \n",
    "\t    (output from calculateStats)\n",
    "\t    nSD (int): How many standard deviations away from the mean should the \n",
    "\t    cutoff point be\n",
    "\t    outfile (str): Where to store the list of pruned subreddits\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\tintermediate = pd.read_csv(subSummaries, index_col=0)\n",
    "\taverage  = intermediate['Total Karma'].mean()\n",
    "\tprint(intermediate['Total Karma'])\n",
    "\tsd = intermediate['Total Karma'].std()\n",
    "\ttoKeep = intermediate['Total Karma'] >= average + nSD * sd\n",
    "\tprint(average + nSD * sd)\n",
    "\tpruned = intermediate[toKeep]\n",
    "\tout = open(outfile, 'w+')\n",
    "\tout.write('\\n'.join(pruned.index))\n",
    "\tout.close()\n",
    "\n",
    "\n",
    "def buildMasterSheet(userSummaries:str, prunedSubList:str, outFile:str):\n",
    "\t\"\"\"Prunes out insignificant subreddits from the userSummaries\n",
    "\t\n",
    "\tArgs:\n",
    "\t    userSummaries (str): file path to raw user summaries\n",
    "\t    prunedSubList (str): path to list of pruned subs\n",
    "\t    outFile (str): file path to outfile\n",
    "\t\"\"\"\n",
    "\tsubs = open(prunedSubList).read().splitlines()\n",
    "\n",
    "\tdef procLine(l):\n",
    "\t\treturn ('\\t'.join(filter(lambda x: x.endswith('.json') or x.split(' ')[0] in subs,l.split('\\t')))).replace('.json','',1)\n",
    "\n",
    "\toutput = map(procLine, sorted(open(userSummaries).read().splitlines()))\n",
    "\topen(outFile, 'w+').write('\\n'.join(output))\n",
    "\n",
    "\n",
    "def pruneSubSummaries(subSummaries, prunedSubList, outfile):\n",
    "\timport pandas as pd\n",
    "\tsummaries = pd.read_csv(subSummaries, index_col=0)\n",
    "\tsubs = set(open(prunedSubList).read().splitlines())\n",
    "\tpruned = [s for s in summaries.index if s in subs]\n",
    "\tsummaries.loc[pruned].to_csv(outfile)\n",
    "\n",
    "\n",
    "def findLinks(masterList:str, subSummaries:str, outfile:str):\n",
    "\t\"\"\"Finds links between subs the users have participated in, the strength of\n",
    "\tthe links are based off on how strong of a presence a member has in any pair of\n",
    "\tsubs they have participated in as deffined by number of standard deviations away\n",
    "\tfrom the mean\n",
    "\t\n",
    "\tArgs:\n",
    "\t    masterList (str): path to masterlist\n",
    "\t    subSummaries (str): path to sub summaries\n",
    "\t    outfile (str): path to outfile\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\tfrom dagm.utils.progressBar import printProgressBar\n",
    "\tsubs = pd.read_csv(subSummaries, index_col=0)\n",
    "\tsubLinks = {s:{} for s in subs.index}\n",
    "\tuserSummaries = open(masterList).read().splitlines()\n",
    "\ttotal = len(userSummaries)\n",
    "\toutput = open(outfile,'w+')\n",
    "\tcounter = 0\n",
    "\tfor userSummary in map(lambda x: x.split('\\t'),userSummaries):\n",
    "\t\t# user = userSummary[0]\n",
    "\t\t# print('\\n\\n',userSummary[0])\n",
    "\t\tdef mapFun(subScore):\n",
    "\t\t\tsub, score = subScore\n",
    "\t\t\tweight = (score - subs.loc[sub]['Mean']) / subs.loc[sub]['SD']\n",
    "\t\t\treturn (sub, weight)\n",
    "\n",
    "\t\tsignificantSubs = list(map(mapFun, map(lambda x: (lambda y: (y[0], int(y[1])))(x.split(' ')), userSummary[1:])))\n",
    "\t\tfor source, weightS in significantSubs:\n",
    "\t\t\tfor dest, weightD in significantSubs:\n",
    "\t\t\t\tif source != dest:\n",
    "\t\t\t\t\tif source not in subLinks[dest]:\n",
    "\t\t\t\t\t\tsubLinks[dest][source] = 0\n",
    "\t\t\t\t\tsubLinks[dest][source] += (weightS * weightD)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter % 1000 == 1:\n",
    "\t\t\tprintProgressBar(counter, total, decimals= 2, length=50)\n",
    "\n",
    "\tfor sub in subLinks:\n",
    "\t\tline = [f'{s} {subLinks[sub][s]}' for s in subLinks[sub]]\n",
    "\t\toutput.write(f'{sub}\\t')\n",
    "\t\toutput.write('\\t'.join(line))\n",
    "\t\toutput.write('\\n')\n",
    "\n",
    "\n",
    "def linkStats(subSummaries:str, rawLinks:str, output:str):\n",
    "\t\"\"\"Compute the standard deviation and mean link weights\n",
    "\t\n",
    "\tArgs:\n",
    "\t    subSummaries (str): path to the subSummaries\n",
    "\t    rawLinks (str): path to the raw link file\n",
    "\t    output (str): path where to save\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport numpy as np\n",
    "\tfrom dagm.utils.progressBar import printProgressBar\n",
    "\tsubs = {s:[] for s in pd.read_csv(subSummaries, index_col=0).index}\n",
    "\tlines = open(rawLinks).read().splitlines()\n",
    "\tacc = 0\n",
    "\tfor data in map(lambda x: list(map(lambda y: y.split(' '), x.split('\\t'))),lines):\n",
    "\t\t# sub = data[0]\n",
    "\t\tfor link, weight in map(lambda x: (x[0],float(x[1])),data[1:]):\n",
    "\t\t\tsubs[link].append(weight)\n",
    "\t\tacc += 1\n",
    "\t\tif acc % 100 == 1:\n",
    "\t\t\tprintProgressBar(acc, len(lines))\n",
    "\tpd.DataFrame({s:{'Mean':np.mean(subs[s]), 'SD':np.std(subs[s])} for s in subs}).T.to_csv(output)\n",
    "\n",
    "\n",
    "def pruneLinks(linkStats:str,prunedSubList:str, rawLinks:str, output:str, nSD=1.5):\n",
    "\t\"\"\"Removes all links that are less than nSD above the mean\n",
    "\t\n",
    "\tArgs:\n",
    "\t    linkStats (str): Path to path containing the statistics for the links\n",
    "\t    prunedSubList (str): list of pruned subreddits\n",
    "\t    rawLinks (str): raw link information\n",
    "\t    output (str): output\n",
    "\t    nSD (float, optional): how many standard deviations away from the mean to set the cuttoff\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\tfrom dagm.utils.progressBar import printProgressBar\n",
    "\tlinkStats = pd.read_csv(linkStats, index_col=0)\n",
    "\tlines = open(rawLinks).read().splitlines()\n",
    "\t# prunedLinks = {s:'' for s in open(prunedSubList).read().splitlines()}\n",
    "\tacc = 0\n",
    "\toutFile = open(output, 'w+')\n",
    "\tfor data in map(lambda x: list(map(lambda y: y.split(' '), x.split('\\t'))),lines):\n",
    "\t\tsub = data[0][0]\n",
    "\t\t# print(sub)\n",
    "\n",
    "\t\tdef filterFun(linkWeight):\n",
    "\t\t\tif len(linkWeight) == 1:\n",
    "\t\t\t\treturn False\n",
    "\t\t\t# print(linkWeight)\n",
    "\t\t\tlink, weight = linkWeight\n",
    "\t\t\t# print(link, weight)\n",
    "\t\t\tcutoff = float(linkStats.loc[link]['Mean'] + nSD* linkStats.loc[link]['SD'])\n",
    "\t\t\t# print(cutoff)\n",
    "\t\t\treturn float(weight) >= cutoff\n",
    "\n",
    "\t\toutFile.write(f'\\n{sub}\\t')\n",
    "\t\toutFile.write('\\t'.join(list(map(' '.join,filter(filterFun, data[1:])))))\n",
    "\t\t\n",
    "\t\tacc += 1\n",
    "\t\tif acc % 100 == 1:\n",
    "\t\t\tprintProgressBar(acc, len(lines), length=50, decimals = 2)\n",
    "\n",
    "\toutFile.close()\n",
    "```\n",
    "\n",
    "#  SVM Model Training\n",
    "## Finding the Best Parameters\n",
    "We used grid search technique to assign 8 different cost value and 6 different gamma value to out SVM model hoping that we can hit an even higher double-fold verification accuracy. However, some value pairs are incredibly time-consuming and the whole grid search only marginally increases our accuracy. As grid search did not help much, we only included it as an experimental cell in our final code.\n",
    "\n",
    "After dimension reduction, there were 3257 users left. We decided to assign first 3000 users to training data set and the last 257 users to evaluate the performance of our SVM model. When we finally managed to obtain a satisfying accuracy after dozens of dimension reduction and grid search trials, we found that default parameters for SVM are actually the best balance between efficiency and accuracy. For maximum prediction accuracy.\n",
    "\n",
    "## Procedure\n",
    "First we train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the tools for our project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the main data frame for SVM training\n",
    "df_training = pd.read_csv('COGS108_DepressedDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_selection = open('cols_4.00SD.csv').read().split(',') # open up the selected label list for dimension reduction\n",
    "df_label_selection.remove('r/happy') # remove the label which is directly linked to no depression \n",
    "df_label_selection.remove('r/depression') # remove the label which is directly linked to depression\n",
    "df_training_indicator = df_training['Depressed'] # seperate the pre-assigned depression indicators from the main data frame\n",
    "df_training_data = df_training[df_label_selection] # select the wanted dimensions according to the selected label list\n",
    "df_training_indicator = df_training_indicator[df_training_data.sum(axis=1) > 0] # discard the indicators linking to the users which have zero karma across the row\n",
    "df_training_data = df_training_data[df_training_data.sum(axis=1) > 0] # discard the users which have zero karma across the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_data_train = df_training_data.head(3000) # pick the first 3000 users for training\n",
    "df_training_indicator_train = df_training_indicator.head(3000)\n",
    "train_data = df_training_data_train.values.astype(int).tolist() # convert data frames to lists that can be accepted by SVM\n",
    "train_indicator = df_training_indicator_train.astype(int).tolist()\n",
    "df_training_data_test = df_training_data.tail(257) # pick the rest 257 users for testing\n",
    "df_training_indicator_test = df_training_indicator.tail(257)\n",
    "test_data = df_training_data_test.values.astype(int).tolist() # convert data frames to lists that can be accepted by SVM\n",
    "test_indicator = df_training_indicator_test.astype(int).tolist()\n",
    "depression = svm.SVC(gamma='auto') # create a SVM object called depression\n",
    "depression.fit(train_data, train_indicator) # train the model for the first time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test the accuracy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70038910505836571"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depression.score(test_data, test_indicator) # test the model with testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86199999999999999"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depression.score(train_data, train_indicator) # double-check the performance of this model on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy and Ethics\n",
    "While conducting our investigation, it is crucial that we adhere to the following ethical guidelines:\n",
    "- Anonymity and confidentiality: We will not collect any personally identifiable data, everything on our dataset is publicly available on reddit.com\n",
    "- By no means, we should harm particular users by harassing/interrogating the users through posts/comments during data collection. \n",
    "- We must only use relevant data such as posts, comments, user profile and a non-biased (in terms of gender, race, ethnicity, age…) machine learning attributes to assess our model. Our results must only aim to address our hypothesis and not ascertain any generalizations about personal/social/political characteristics of certain user groups that are beyond the scope of our investigation. \n",
    "\n",
    "Regarding permission to use this data, before our gathered our data from Reddit.com, we registered a script for Reddit and were required to adhere to a rigorous API terms and data privacy policy as defined [Here](https://www.reddit.com/wiki/api).The biggest privacy concerns regarding our datasets is the anonymity of users and any possibly traces that uncover any personal information such as address, sex, age of the users. To handle this issue and eradicate this threat, we scraped off the usernames in our final dataset. Although it is theoretically possible to cross reference our data with the information available on reddit to deduce which users were included, this is beyond the scope of our privacy concerns as this information is already publicly available. \t\t\t\n",
    "In response to the potential biases in your dataset in terms of who it composes, and how it was collected, our dataset may be problematic in terms of it allowing for equitable analysis. Namely, as previously discussed, we are aware that the the average Reddit user can be inferred as a white male college student from the United States. Namely, nearly 70% of Reddit users are male, and 93% of users are under 50 years old, of which 64% of users are between 18-26. It, therefore, comes as little surprise that more than 50% of users are currently students. Considering the fact that depression affects people regardless of their age, ethnicity, location... It is crucial we clarify that our analysis is nowhere representative of and applicable to the general human population. Namely, our analysis draws onto a smaller scope of the typical US college, male student population. It is worth pointing out that our dataset inherently is biased towards a certain population and thereby excluding the rest. Yet just like human psychology, it is often difficult to generalise a phenomenon that is applicable to the majority of the population. What we can do though, in the future, is that we can choose a dataset in another environment that is more inclusive of different sub-categories of population across different genders and ranges of age, for instance we can utilise census data or even conduct our own survey. \n",
    "\n",
    "# Conclusions and Discussion\n",
    "At this point we have developed a model with above trivial accuracy, while on a purely intellectual and technical level this is a satisfactory accomplishment, we noted in our original proposal that we were interested in using this technology to develop a tool that had the potential to provide some tangible benefit from society. Specifically, we wanted to develop a model that could detect depressive tendencies in subjects based solely on their online activity. We will now demonstrate the viability of this: We built two new datasets, they contain the user summaries of all participants in r/SuicideWatch and  r/Corgi respectively. We are hypothesizing that the vast majority of participants in r/SuicideWatch will exhibit depressive tendencies and therefore a majority of users will be flagged by our model, conversely, we are hypothesizing that a majority of users in r/Corgi will not exhibit depressive tendencies.\n",
    "\n",
    "## Summary:\n",
    "For our project we wanted to evaluate the different subreddits that people follow to see if it would reflect the likelihood of them having depression or displaying depressive traits. Evaluating something as depression can be abstract because it can be difficult for people themselves to identify whether or not they are depressed. Therefore, we used similarities in social media use to find potential correlations between people with depressive tendencies.\n",
    "\n",
    "To this end, we first selected a few subreddits, such as r/SuicideWatch, in which we presume that the majority of participants are depressed or show signs of depression. Because there are participants who are not depressed, we chose several different subreddits with a focus on depression to decrease the risk of inaccurate predictions. We then also chose several subreddits with interests geared away from people with depression, such as r/happy. By comparing users’ participation in both of these sets, we were able to make predictions about the chance of their having depressive tendencies.\n",
    "\n",
    "The most significant part of training the model was in finding which subreddits most strongly correlated with depressive tendencies, and the opposite. By doing so, we were able to predict based on user data whether or not a user is likely to have depressive traits, with a success rate of over 70%. \n",
    "\n",
    "Our aim in creating this model is to take a step toward analysing how social media use is a reflection of one’s mental health. Reddit being an anonymous forum, many people find a way to express themselves that is difficult without wearing the “mask” that the site provides. Our hope is that people will gain an understanding of what internet habits are healthy, and to consider, if they did not know it before, that they may have depressive or unhealthy tendencies. There are many limits to our data and model; for example, as emphasized, Reddit’s demographic is mostly young, white males. Therefore the depressive habits we have found may correlate most strongly to one particular subsection of the world’s population. Analyzing a platform with more equal distribution or emphasis on other subsets may help flesh out the model. Our data also does not include significant points such as when a user is active, how frequently they are active on the site, and other traits that may be significant in representing how people with depressive tendencies. But for a model that analyzes based solely on personal interests and post activity, we were able to find strong correlations between subreddits whose users display concerning common traits related to their mental health. \n",
    "\n",
    "One of the most important facets of data science is respecting the privacy of the people from whom we collect data. This holds in many other departments, but especially one as sensitive and personal as mental health. In no way is our project meant to reflect poorly on the users we analyzed; rather, with the wealth of public information available online, we are hoping to demonstrate how this data can be used to benefit many and raise awareness of the correlations between depression and social media use. As technology develops, humans will become exposed to many new and different ways to express ourselves; our hope with this project is that we will similarly be able to analyze like patterns of behavior between people and better understand our own traits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting from here, it's all about the new data \n",
    "df_SuicideWatch = pd.read_csv('COGS108_SuicideWatch.csv') # load SuicideWatch user data into a data frame. The majority of the users are expected to be depressed\n",
    "df_Corgi = pd.read_csv('COGS108_Corgi.csv') # load Corgi user data into a data frame. The majority of the users are expected to be not depressed\n",
    "df_SuicideWatch_data = df_SuicideWatch[df_label_selection] # select the wanted dimensions according to the selected label list\n",
    "df_Corgi_data = df_Corgi[df_label_selection]\n",
    "df_SuicideWatch_data = df_SuicideWatch_data[df_SuicideWatch_data.sum(axis=1) > 0] # discard the users which have zero karma across the row\n",
    "df_Corgi_data = df_Corgi_data[df_Corgi_data.sum(axis=1) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the whole reduced-dimension training set to train our SVM model again \n",
    "#for maximum performance\n",
    "whole_data = df_training_data.values.astype(int).tolist() # convert all user data to list\n",
    "whole_indicator = df_training_indicator.astype(int).tolist() # convert all indicators to list\n",
    "depression.fit(whole_data, whole_indicator) # train SVM again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SuicideWatch_prediction = depression.predict(df_SuicideWatch_data) # predict the depression indicators for SuicideWatch users\n",
    "df_Corgi_prediction = depression.predict(df_Corgi_data) # predict the depression indicators for Corgi users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SuicideWatch_score_whole = sum(df_SuicideWatch_prediction)/len(df_SuicideWatch)\n",
    "Corgi_score_whole = sum(df_Corgi_prediction)/len(df_Corgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91852433281004708"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SuicideWatch_score_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18743366588834642"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Corgi_score_whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe this is fairly strong evidence in favor of our hypothesis as our model is working as expected and at this point could be used to flag potentially depressed individuals in order to perhaps help direct them to mental health specialists as discussed in our research question.\n",
    "\n",
    "\n",
    "We realize that depression is a sensitive topic for many but we also believe that it is important to be aware of the signs in order to help treat it. As we enter into the digital age, many people tend to spend their time online and their web history can give insight into their personality. Although there are similar models out there using this type of algorithm to bombard the user feed with information that could be encouraging that behavior. While there is more research needed to be done in order to evaluate the depths of these results, our model gives a starting point to taking steps in identifying these kind of behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apendix: Gridsearch\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(train_data)\n",
    "train_data_pca = pca.transform(train_data)\n",
    "test_data_pca = pca.transform(test_data)\n",
    "params_dict = {\"C\":np.logspace(-3, 11, num=8), \"gamma\":np.logspace(-11, -1, num=6)}\n",
    "depression = svm.SVC(kernel='rbf')\n",
    "search = GridSearchCV(estimator=depression, param_grid=params_dict)\n",
    "search.fit(train_data, train_indicator)\n",
    "search.score(test_data, test_indicator)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
