{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Quiz | Kai Hong Pang | khpang@ucsd.edu | +1 858 568 4143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Data Science Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1[Basic] Can you briefly explain Backpropagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's an algorithm used to determine adjustments to weights and biases of a training example in the network, such that relative proportions to changes to weights and bias of the network creates the largest decrease in cost. \n",
    "\n",
    "Namely, since each layers of neurons are interconnected with weights and activation, back propagation utilises the chain rule and works its way backwards, starting from output layer and computes the gradient of each neuron w.r.t to all its incoming, outgoing connections and the cost function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2[Basic] There are three types of Gradient Descent, please list them out and explain the pros and cons of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch  \n",
    "\n",
    "PROS\n",
    "   * Convergence is stable and relatively straight forward when dataset is not suuuuper large\n",
    "\n",
    "CONS\n",
    "  * Computational, memory expensive when dataset is large\n",
    "  \n",
    "  * Not every training sample contribute equal influence/insight to the update rule. It may be that convergence of the minimum already occurs before the algorithm terminates, so time and efforts may be unnecessarily wasted \n",
    "\n",
    "\n",
    "## Stochastic\n",
    "\n",
    "PROS\n",
    "\n",
    "   * May provide more rapid decrease in cost compared to Batch when dataset is large, as updates to weights are made with every training sample\n",
    "\n",
    "CONS\n",
    "\n",
    "   * Process of decreasing cost across # of epochs is noisiest out of the 3\n",
    "   * Error rate may fluctuate as the gradient is computed with each individual shuffled data point\n",
    "   * May not converge to a very definitive minimum at all\n",
    "\n",
    "\n",
    "## Mini Batch\n",
    "\n",
    "PROS\n",
    "\n",
    "   * Balances the stable convergence of Batch descent and efficiency of Stochastic descent as it uses less samples than Batch to compute gradient\n",
    "   \n",
    "CONS\n",
    "\n",
    "   * Cost decrease remains noisy as the gradient at each update is computed using a relatively small subset of training samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 [Basic] Why is Zero Weight Initialization not recommended? Please explain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all weights are initialised to 0, then having multiple neurons in any hidden layer is redundant cause then learning will be homogenous across all neurons and thus the network is like 'stagnated'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 [Intermediate] What is Batch Normalization and why should we use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization standardizes (or normalizes) output data from any activation function in individual layers of the neural network. Normalizing data helps reduce the time of learning, namely when calculating gradients and also prevents exploding/vanishing gradients that cascade deeper down into the neurat network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 [Intermediate] How to pick activation functions and what’s the difference among them? (Just list out those you have used and provide reasons for why you picked them.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be honest, I am mainly exposed to sigmoid activation function, mainly because it is a familiar concept taught in logistic regression. But out of curiosity I also looked up the problem of a 'vanishing' gradient when the output values in the sigmoid function is close to 1,0. One solution is ReLu, a technique that takes the max(activation output, 0). However, I've yet to experiment and dive deeper into this activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 [Intermediate] RNN is well known to perform well in sequential data, please explain why. You can explain the structure of the network. Can you make a comparison with Time Series models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of RNN is based on its output, like a self feeding loop. It works well with sequential data because often the prediction in a sequence, let's say the word suggestion in a sentence, often depends on the pre-existing words on sentence. A very common application of RNN in time series data would be the prediction of stocks price, since the stock price today is somewhat related to the closing stock price of yesterday. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## B. Coding Section\n",
    "\n",
    "Q1       [Basic] You are given a sorted array, please write a function to remove the duplicates in-place such that each element appears only ONCE and return the new length. Note: Please do not allocate extra space for another array. You must modify the input array in-place with O(1) extra memory.\n",
    "—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though basic, this question actually took me a while! The first thought that came into my mind is to have 2 pointers iterate through the array (probably a while loop), and use the del function whenever the pointer point to the same number. But I quickly realised that if I delete the elements inside the loop, then it just gets super messy. So this approach swaps the number instead and at the end shrinks the list so that only the unique elements remain. Then I return the length of that list  \n",
    "\n",
    "    ## def(func)\n",
    "    \n",
    "    ## ERROR CHECK: return the len(lst) if length of list is < 2\n",
    "    \n",
    "    ## set up 'pointer' variable = 0\n",
    "    \n",
    "    ## enumerate through the list so I can keep track of both the index and its value\n",
    "        ## if there's a duplicate, then simply continue to the next iteration\n",
    "        \n",
    "        ## else, I will increment pointer and swap the current lst[pointer] to val\n",
    "\n",
    "    ## OUTSIDE the loop \n",
    "        ## To remove the duplicates in place, use list splice assignment i.e. lst = list[:pointer]\n",
    "     \n",
    "    # return len(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2       [Basic] You are given an array which elements are sorted in ascending order, please convert it to height-balanced Binary Search Tree.\n",
    "\n",
    "\n",
    "Example: Given sorted array: [-10, -3, 0, 5, 9]\n",
    "One possible answer: [0, -3, 9, -10, null, 5]\n",
    "Representing the following BST:\n",
    "\n",
    "                                    0\n",
    "                                 /      \\\n",
    "                              -3       9\n",
    "                             /         /\n",
    "                         -10       5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HONEST DISCLAIMER. \n",
    "I am not very comfortable with recursion, so I am still working on improving my intution on the recursive calls and call stacks. \n",
    "\n",
    "But here's my attempt! \n",
    "\n",
    "The first thought that came to my mind when I read the question is that I am aware that for the BST property to hold, the median of a sorted array has to be the root note of the BST, as the BST specifies that the left subtree at any given point is smaller than its root and vice versa for the right subtree.\n",
    "\n",
    "\n",
    "    ## def build_bst(sorted_arr, start_pos, end_pos):\n",
    "        ## ERROR CHECK: if len(sorted_arr) = 0 or sorted_arr == null, return\n",
    "        \n",
    "        ## BASE CASE:\n",
    "            perhaps if start >= end, then return null (similar to the base case in sorting algorithms)\n",
    "                \n",
    "        ## RECURSIVE STEP:\n",
    "            find the midpoint, using floor division may be a good idea in case length of sorted_arr is even \n",
    "          \n",
    "            build the root of the BST\n",
    "          \n",
    "            # create the left and right subtrees of the root (change the start_pos, end_pos params)\n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3       [Intermediate] You are given a Binary Search Tree, please write a function that finds the Kth smallest element in it. Assume K is always valid and the value of it is always in between 1 and BST’s total elements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## The first thought that came to my mind is in-order traversal. You start from the bottom-left corner of the BST, then traverse through the BST. \n",
    "\n",
    "    ## I am thinking to use a list to store my results, and throw a counter initialised at 0 into the in-order traversal algorithm\n",
    "\n",
    "    ## At every traversal to a node (not null), I will increment the counter. I will simply set an if condition and return the node value when counter == k.  This node value will be the K-th smallest number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4       [Intermediate] You are given a non-empty array of integers, return the K most frequent elements\n",
    "\n",
    "Example:\n",
    "Input: nums = [1,1,1,2,2,3], k = 2\n",
    "Output: [1,2]\n",
    "\n",
    "Example:\n",
    "Input: nums = [1], k = 1\n",
    "Output: [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## First initial thought is to use a hashmap, or dictionary in Python to keep track of the # of occurences of each number. \n",
    "    \n",
    "    ## I will iterate through the array of integers to add them into a dictionary, where the key of the dictionary is the integer itself and the value is the # occurence. Thus the (key:value) pair of the dictionary will represent the (integer: # of counts). To find the K most frequent elements, I will sort by the values in descending order, and I will simply the first K keys of the dictionary into a list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THANKS FOR YOUR TIME!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
